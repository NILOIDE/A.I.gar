testing:

Default: Avg - 308.4 + 311.7 + 309.2 + 308 + 312: 309.86meanMass (mM)
	 Max - 602   + 644   + 653   + 628 + 645: max- 653maxMass

All 1M steps

q-learning:

	gpu node - 3x - 0.5s per update

	gpu node with wait time 10 and batch size 320 -3x - 0.26s per update - 309.8mM

	gpu node with wait time 100 and batch size 3200 - 1x -

	gpu wt 1000 bs 32000 - 1x -

	gpu node with 2 gpus waitTime 100 and batch size 3200 - 1x-

	2 gpu wt 1000 bs 32000 - 1x - 


        no exp rep - 5 tests:

	relu - 5 tests: 312.4mM

		relu - 5 more tests:310.2mM

	vs 1 greedy - 3 tests:

	alpha 0.0001 - 3 tests:

	alpha 0.0005 - 3 tests:

	Nil:
	FRAME_SKIP_RATE = 5 x4
	FRAME_SKIP_RATE = 7 x4
	FRAME_SKIP_RATE = 11 x4
	GRID_SQUARES_PER_FOV = 9 x3 - 311.3mM
	GRID_SQUARES_PER_FOV = 13 x3 - 305.1mM
	EPSILON_DECAY = 0.99997 x3
	EPSILON_DECAY = 0.99993 x3
	DISCOUNT = 0.92 x3
	DISCOUNT = 0.88 x3
	MEMORY_CAPACITY = 10000 x3 - 293.2mM
	MEMORY_CAPACITY = 1000 x3 - 255mM 



	

Actor Critic:

	New Actor (without divide by sigma squared and without split/eject): 8x - 143.9mM, 5/8 gave shitty result(mM of 20, rest has mM of around 280)

	New Actor no actor replay: 5x - 6.3mM, all failed

	New Actor type standard: 5x - 6.2mM, all failed

	New vs greedy: 3x - 135+224+204=188mM, none seemed to fail

	New alpha policy 0.00025 - 5x - 170.8mM, 2/5 failed, 290mMsuccess, 6.2mM on fail

	New alpha policy 0.001 - 5x - 60.6mM, 4/5 failed, 277mM success, 6.2mM on fail

	New noise decay 0.99994 - 5x - 111.9mM, 3/5 failed, 270mM success, 6.2mM on fail

	New noise decay 0.99996 - 5x - 112.mM, 3/5 failed, 262mM success, 6.x on fail

	old AC standard - 5 tests: 6.3mM



	Nil:
	Noise decay: 0.99993 x3 

	Noise decay: 0.99997 x3

	alpha actor 0.001 x3

	alpha actor 0.00025 x3

	hidden neurons: 200 all layers x5

LSTM: - 
	lstm pure - 5 tests: 57.2mM

	lstm no exp rep - 3 tests: 13.3mM

	lstm vs greedy - 3 tests: 15mM

	lstm trace length 15 - 5 tests:

	lstm trace length 15 minimum 10 - 5 tests -

	lstm ACTIVATION_FUNC_LSTM="sigmoid" - 5 tests - 105.7mM

	lstm act func lstm = "elu" - 5 tests -135.6mM - it seems to learn much faster on elu, maybe sigmoid activation functions need more training time/steps to shine.


